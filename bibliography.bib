@inproceedings{Yang_2024_CVPR,
author = {Yang, Yue and Sun, Fan-Yun and Weihs, Luca and VanderBilt, Eli and Herrasti, Alvaro and Han, Winson and Wu, Jiajun and Haber, Nick and Krishna, Ranjay and Liu, Lingjie and Callison-Burch, Chris and Yatskar, Mark and Kembhavi, Aniruddha and Clark, Christopher},
title = {{Holodeck: Language Guided Generation of {3D} Embodied {AI} Environments}},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2024},
pages = {16227--16237},
url = {https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_CVPR_2024_paper.html},
urldate = {2026-01-29},
annote  = {Holodeck presents a system that converts a natural-language prompt into an interactive 3D environment for embodied AI. The approach uses a large language model to draft object inventories and spatial relations, then optimizes object placement to satisfy those constraints while populating scenes with assets from Objaverse. The paper contributes a full prompt-to-scene pipeline, human preference studies that outperform procedural baselines on residential scenes, and demonstrations that agents can be trained in the generated environments.
This is a credible source because it is a peer-reviewed CVPR 2024 paper with a clear methodology, extensive evaluations, and transparent comparisons to baselines, which strengthens confidence in the reported gains. It is relevant to research on language-guided environment generation and controllable simulation.
This connects to the Holodeck-like prompt-to-world/spec generation concept by showing how constraints can be explicitly represented, solved deterministically, and evaluated in simulated environments to assess downstream agent performance.}
}

@inproceedings{Deitke_2022_NeurIPS,
author = {Deitke, Matt and VanderBilt, Eli and Herrasti, Alvaro and Weihs, Luca and Ehsani, Kiana and Salvador, Jordi and Han, Winson and Kolve, Eric and Kembhavi, Aniruddha and Mottaghi, Roozbeh},
title = {{ProcTHOR: Large-Scale Embodied AI Using Procedural Generation}},
booktitle = {Advances in Neural Information Processing Systems},
volume = {35},
year = {2022},
url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/27c546ab1e4f1d7d638e6a8dfbad9a07-Abstract-Conference.html},
urldate = {2026-01-29},
annote  = {ProcTHOR introduces a procedural generation framework for creating large numbers of diverse, fully interactive indoor environments for embodied AI. The system samples floorplans, populates them with a large library of interactive objects, and randomizes layouts, materials, and lighting to scale scene diversity. The paper contributes both the ProcTHOR generation pipeline and a complementary artist-designed evaluation set (ArchitecTHOR), and it reports that training on large procedurally generated collections improves performance and generalization across multiple embodied AI benchmarks.
This is a strong scholarly source because it is a peer-reviewed NeurIPS 2022 contribution with extensive empirical evidence and a clear methodological advance in scalable environment creation, which is central to embodied AI research. It also clarifies trade-offs between procedural controllability and realism that matter when selecting generation settings.
This connects to the Holodeck-like concept by providing a concrete example of deterministic compilation from structured scene specifications into simulated environments and by outlining evaluation protocols for those environments in embodied tasks.}
}

@InProceedings{hoellein2023text2room,
  author    = {H{\"o}llein, Lukas and Cao, Ang and Owens, Andrew and Johnson, Justin and Nie{\ss}ner, Matthias},
  title     = {{Text2Room}: Extracting {Textured} {3D} {Meshes} from {2D} {Text} to {Image} {Models}},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2023},
  pages     = {7909--7920}
}

@inproceedings{Zhang_2024_CVPR_SceneDreamer,
author = {Zhang, Songchun and Zhang, Yibo and Zheng, Quan and Ma, Rui and Hua, Wei and Bao, Hujun and Xu, Weiwei and Zou, Changqing},
title = {{3D-SceneDreamer}: Text-Driven {3D}-Consistent Scene Generation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
publisher = {IEEE/CVF},
month = {June},
year = {2024},
pages = {10170--10180},
url = {https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_3D-SceneDreamer_Text-Driven_3D-Consistent_Scene_Generation_CVPR_2024_paper.html},
urldate = {2026-02-19},
annote  = {3D-SceneDreamer targets text-driven scene synthesis with explicit emphasis on preserving 3D consistency across viewpoints. The paper identifies a core weakness in prior warping-and-inpainting pipelines: cumulative geometric and appearance errors. To address this, the authors use a tri-plane feature NeRF as a global scene representation and introduce a generative refinement stage that repeatedly aggregates global 3D context while expanding local content. The resulting pipeline supports broader camera motion and improves consistency and visual quality in both indoor and outdoor settings.
This is a credible source because it is a peer-reviewed CVPR 2024 publication with a clear technical decomposition, ablation studies, and comparative evaluation against recent text-to-3D scene baselines. The paper is relevant to this project because it provides concrete design patterns for balancing global scene coherence with iterative local generation.
For a Holodeck-style system, the main takeaway is architectural: represent global structure in a stable intermediate form, then refine in stages rather than generating everything at once. That aligns with a deterministic compile pipeline where a minimally playable world appears first and fidelity upgrades arrive in later phases under explicit validation and budget constraints.}
}

@inproceedings{Gao_2024_CVPR_GraphDreamer,
author = {Gao, Gege and Liu, Weiyang and Chen, Anpei and Geiger, Andreas and Sch{\"o}lkopf, Bernhard},
title = {{GraphDreamer}: Compositional {3D} Scene Synthesis from Scene Graphs},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
publisher = {IEEE/CVF},
month = {June},
year = {2024},
pages = {21295--21304},
doi = {10.1109/CVPR52733.2024.02012},
url = {https://openaccess.thecvf.com/content/CVPR2024/html/Gao_GraphDreamer_Compositional_3D_Scene_Synthesis_from_Scene_Graphs_CVPR_2024_paper.html},
urldate = {2026-02-19}
}
