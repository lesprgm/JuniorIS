@inproceedings{Chen_2023_WACV,
author = {Chen, Boris and Ziai, Amir and Tucker, Rebecca S. and Xie, Yuchen},
title = {Match Cutting: Finding Cuts With Smooth Visual Transitions},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {January},
year = {2023},
pages = {2115--2125},
url = {https://openaccess.thecvf.com/content/WACV2023/html/Chen_Match_Cutting_Finding_Cuts_With_Smooth_Visual_Transitions_WACV_2023_paper.html},
urldate = {2026-01-29},
annote  = {This paper targets automatic discovery of high-quality match cuts, where two shots align in framing, composition, or action to create a smooth transition. The authors present a modular pipeline that generates candidate shot pairs and ranks them using visual, audio, and audio-visual descriptors, and they compare classification and metric-learning models on a labeled dataset of roughly 20,000 pairs. A key contribution is a scalable, reproducible benchmark with released data, learned embeddings, and evaluation metrics for match-cut retrieval.
As a peer-reviewed WACV 2023 publication, the work is credible and methodologically transparent, with controlled experiments and publicly released artifacts that support verification and reuse. It is relevant because it operationalizes “smooth visual transition” into measurable features and evaluation protocols that can be adopted directly in new systems.
This connects to the MatchCut Compiler concept, which aims to automatically score candidate match cuts using motion, shape, framing, and semantic similarity, and to provide concise explanations ("receipts") and ranked alternatives for editors.}
}

@inproceedings{Pardo_2022_ECCV,
author = {Pardo, Alejandro and Caba Heilbron, Fabian and Le\'on Alc\'azar, Juan and Thabet, Ali and Ghanem, Bernard},
title = {MovieCuts: A New Dataset and Benchmark for Cut Type Recognition},
booktitle = {Computer Vision -- ECCV 2022},
series = {Lecture Notes in Computer Science},
volume = {13667},
pages = {668--685},
year = {2022},
publisher = {Springer},
doi = {10.1007/978-3-031-20071-7_39},
url = {https://link.springer.com/chapter/10.1007/978-3-031-20071-7_39},
urldate = {2026-01-29}
}

@inproceedings{Chen_2021_CVPR,
author = {Chen, Shixing and Nie, Xiaohan and Fan, David and Zhang, Dongqing and Bhat, Vimal and Hamid, Raffay},
title = {Shot Contrastive Self-Supervised Learning for Scene Boundary Detection},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2021},
pages = {9796--9805},
url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Shot_Contrastive_Self-Supervised_Learning_for_Scene_Boundary_Detection_CVPR_2021_paper.html},
urldate = {2026-01-29}
}

@inproceedings{Yang_2024_CVPR,
author = {Yang, Yue and Sun, Fan-Yun and Weihs, Luca and VanderBilt, Eli and Herrasti, Alvaro and Han, Winson and Wu, Jiajun and Haber, Nick and Krishna, Ranjay and Liu, Lingjie and Callison-Burch, Chris and Yatskar, Mark and Kembhavi, Aniruddha and Clark, Christopher},
title = {{Holodeck: Language Guided Generation of {3D} Embodied {AI} Environments}},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2024},
pages = {16227--16237},
url = {https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_CVPR_2024_paper.html},
urldate = {2026-01-29},
annote  = {Holodeck presents a system that converts a natural-language prompt into an interactive 3D environment for embodied AI. The approach uses a large language model to draft object inventories and spatial relations, then optimizes object placement to satisfy those constraints while populating scenes with assets from Objaverse. The paper contributes a full prompt-to-scene pipeline, human preference studies that outperform procedural baselines on residential scenes, and demonstrations that agents can be trained in the generated environments.
This is a credible source because it is a peer-reviewed CVPR 2024 paper with a clear methodology, extensive evaluations, and transparent comparisons to baselines, which strengthens confidence in the reported gains. It is relevant to research on language-guided environment generation and controllable simulation.
This connects to the Holodeck-like prompt-to-world/spec generation concept by showing how constraints can be explicitly represented, solved deterministically, and evaluated in simulated environments to assess downstream agent performance.}
}

@inproceedings{Deitke_2022_NeurIPS,
author = {Deitke, Matt and VanderBilt, Eli and Herrasti, Alvaro and Weihs, Luca and Ehsani, Kiana and Salvador, Jordi and Han, Winson and Kolve, Eric and Kembhavi, Aniruddha and Mottaghi, Roozbeh},
title = {{ProcTHOR: Large-Scale Embodied AI Using Procedural Generation}},
booktitle = {Advances in Neural Information Processing Systems},
volume = {35},
year = {2022},
url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/27c546ab1e4f1d7d638e6a8dfbad9a07-Abstract-Conference.html},
urldate = {2026-01-29},
annote  = {ProcTHOR introduces a procedural generation framework for creating large numbers of diverse, fully interactive indoor environments for embodied AI. The system samples floorplans, populates them with a large library of interactive objects, and randomizes layouts, materials, and lighting to scale scene diversity. The paper contributes both the ProcTHOR generation pipeline and a complementary artist-designed evaluation set (ArchitecTHOR), and it reports that training on large procedurally generated collections improves performance and generalization across multiple embodied AI benchmarks.
This is a strong scholarly source because it is a peer-reviewed NeurIPS 2022 contribution with extensive empirical evidence and a clear methodological advance in scalable environment creation, which is central to embodied AI research. It also clarifies trade-offs between procedural controllability and realism that matter when selecting generation settings.
This connects to the Holodeck-like concept by providing a concrete example of deterministic compilation from structured scene specifications into simulated environments and by outlining evaluation protocols for those environments in embodied tasks.}
}

@InProceedings{hoellein2023text2room,
  author    = {H{\"o}llein, Lukas and Cao, Ang and Owens, Andrew and Johnson, Justin and Nie{\ss}ner, Matthias},
  title     = {{Text2Room}: Extracting {Textured} {3D} {Meshes} from {2D} {Text} to {Image} {Models}},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2023},
  pages     = {7909--7920}
}
